{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.6 逻辑回归参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "建立 LogisticRegression 对象时，可以设置很多参数，设置正则化时的参数 `penalty` 和 `C`，设置类型权重的参数 `class_weight` 最为常用。下面我们在逻辑回归中进行调参，看能否优化模型效果。\n",
    "\n",
    "过度拟合是指模型复杂度过高，导致所选模型对已知数据（训练集）预测得很好，但对未知数据（测试集）预测很差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/9d08336320d54a88a6caeb616a96f83873493388d076408d9dc8eda17cbc4e1f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "关于模型拟合的直观展示如下：\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e40d57e432bd461f8d97419a907cf665348c12c37e1742ac8fe3cb3b7f52ff5b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "所谓正则化，就是通过在模型中添加一些惩罚项或约束条件来控制模型复杂度，达到减轻过度拟合的目的。正则化常用的为 $L_1$ 和 $L_2$ 正则化，其中存在正则化系数 $\\lambda (\\lambda>0)$ 为依赖于 `C` 的常数，正则化系数 $\\lambda$ 越大则惩罚力度越大。\n",
    "- `penalty` -- 可设为 `l1` 或 `l2` ，分别代表 $L_1$ 和 $L_2$ 正则化，默认为 `l2`。\n",
    "- ` C` --  `C` 为正则化系数 $\\lambda$  的倒数，必须为正数，默认为 1。值越小，代表正则化越强。\n",
    "- `class_weight` -- 默认为 `None`，可设置为 `balanced`，即根据训练样本量来分配权重。某种类型样本量越多，则权重越低，样本量越少，则权重越高。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 实训任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 建立一个 LogisticRegression 对象，命名为 `lr`，要求使用 `l1` 正则化，参数 `C` 设置为 `0.6`，参数 `class_weight` 设置为 `balanced`。\n",
    "- 对 `lr` 对象调用 `fit` 方法，带入训练集 `x_train, y_train` 进行训练\n",
    "- 对训练好的 `lr` 模型调用 `predict_proba` 方法,带入测试集 `x_test` 进行预测，将结果保存到变量 `y_predict` 中\n",
    "- 调用 `roc_auc_score` 方法，将 `y_test，y_predict` 作为输入参数，求出测试集准确率值，将结果赋予 `test_auc`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，对模型进行三种参数的调整后，准确率有了些许的提升。除了在模型参数上进行调整外，还可以在数据集本身进行一些优化，标准化和离散化是两种比较常用的技巧。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归模型test auc:\n",
      "0.8776952288366846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_table('dataset11(不良记录).txt', sep='\\t')\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=33, stratify=y)\n",
    "\n",
    "# 建立一个LogisticRegression对象，命名为lr\n",
    "lr = LogisticRegression(penalty='l1', C=0.6, class_weight='balanced', solver='liblinear')\n",
    "\n",
    "# 对lr对象调用fit方法，带入训练集x_train, y_train进行训练\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# 对训练好的lr模型调用predict_proba方法\n",
    "y_predict = lr.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# 调用roc_auc_score方法\n",
    "test_auc = roc_auc_score(y_test, y_predict)\n",
    "\n",
    "print('逻辑回归模型test auc:')\n",
    "print(test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 问题：上述三种参数的调整对于模型性能提升的贡献分别是多少呢？请在下方进行消融实验 （ablation study）验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认参数模型 AUC: 0.6493349350319194\n",
      "仅调整 penalty 参数模型 AUC: 0.872667553710911 提升百分比: 34.394055614459305\n",
      "仅调整 C 参数模型 AUC: 0.6468228704664456 提升百分比: -0.38686730529142543\n",
      "仅调整 class_weight 参数模型 AUC: 0.7792301808893791 提升百分比: 20.00435196838353\n",
      "所有参数调整后的模型 AUC: 0.8777189597251362 提升百分比: 35.171990966725076\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#消融实验:它通过逐步去除或禁用模型的特定部分，观察性能的变化，以判断每个部分对整体模型性能的贡献\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_table('dataset11(不良记录).txt', sep='\\t')\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=33, stratify=y)\n",
    "\n",
    "# 使用默认参数训练模型\n",
    "lr_default = LogisticRegression()\n",
    "lr_default.fit(x_train, y_train)\n",
    "y_predict_default = lr_default.predict_proba(x_test)[:, 1]\n",
    "auc_default = roc_auc_score(y_test, y_predict_default)\n",
    "\n",
    "# 仅调整 penalty 参数为 l1\n",
    "lr_penalty = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lr_penalty.fit(x_train, y_train)\n",
    "y_predict_penalty = lr_penalty.predict_proba(x_test)[:, 1]\n",
    "auc_penalty = roc_auc_score(y_test, y_predict_penalty)\n",
    "\n",
    "# 仅调整 C 参数为 0.6\n",
    "lr_C = LogisticRegression(C=0.6)\n",
    "lr_C.fit(x_train, y_train)\n",
    "y_predict_C = lr_C.predict_proba(x_test)[:, 1]\n",
    "auc_C = roc_auc_score(y_test, y_predict_C)\n",
    "\n",
    "# 仅调整 class_weight 参数为 balanced\n",
    "lr_class_weight = LogisticRegression(class_weight='balanced')\n",
    "lr_class_weight.fit(x_train, y_train)\n",
    "y_predict_class_weight = lr_class_weight.predict_proba(x_test)[:, 1]\n",
    "auc_class_weight = roc_auc_score(y_test, y_predict_class_weight)\n",
    "\n",
    "# 使用所有调整后的参数训练模型\n",
    "lr_all = LogisticRegression(penalty='l1', C=0.6, class_weight='balanced', solver='liblinear')\n",
    "lr_all.fit(x_train, y_train)\n",
    "y_predict_all = lr_all.predict_proba(x_test)[:, 1]\n",
    "auc_all = roc_auc_score(y_test, y_predict_all)\n",
    "\n",
    "# 计算提升百分比\n",
    "def calculate_improvement(default_auc, adjusted_auc):\n",
    "    return (adjusted_auc - default_auc) / default_auc * 100\n",
    "\n",
    "improvement_penalty = calculate_improvement(auc_default, auc_penalty)\n",
    "improvement_C = calculate_improvement(auc_default, auc_C)\n",
    "improvement_class_weight = calculate_improvement(auc_default, auc_class_weight)\n",
    "improvement_all = calculate_improvement(auc_default, auc_all)\n",
    "\n",
    "print('默认参数模型 AUC:', auc_default)\n",
    "print('仅调整 penalty 参数模型 AUC:', auc_penalty, '提升百分比:', improvement_penalty)\n",
    "print('仅调整 C 参数模型 AUC:', auc_C, '提升百分比:', improvement_C)\n",
    "print('仅调整 class_weight 参数模型 AUC:', auc_class_weight, '提升百分比:', improvement_class_weight)\n",
    "print('所有参数调整后的模型 AUC:', auc_all, '提升百分比:', improvement_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从消融实验可以得出：调整 penalty 参数模型提升最为明显，但是同时调整三个参数提升比率和仅仅调整oenalty参数模型相差不大，因此继续进行实验."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认参数模型 AUC: 0.6493349350319194\n",
      "所有参数调整后的模型 AUC: 0.8777065487432449 提升百分比: 35.17007962925935\n",
      "仅不调整 penalty 参数模型 AUC: 0.8633726829651499 提升百分比: 32.96261087857671\n",
      "仅不调整 C 参数模型 AUC: 0.8776728617923968 提升百分比: 35.1648917132809\n",
      "仅不调整 class_weight 参数模型 AUC: 0.8727084690358277 提升百分比: 34.400356726983716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_table('dataset11(不良记录).txt', sep='\\t')\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=33, stratify=y)\n",
    "\n",
    "# 使用默认参数训练模型\n",
    "lr_default = LogisticRegression()\n",
    "lr_default.fit(x_train, y_train)\n",
    "y_predict_default = lr_default.predict_proba(x_test)[:, 1]\n",
    "auc_default = roc_auc_score(y_test, y_predict_default)\n",
    "\n",
    "# 使用所有调整后的参数训练模型\n",
    "lr_all = LogisticRegression(penalty='l1', C=0.6, class_weight='balanced', solver='liblinear')\n",
    "lr_all.fit(x_train, y_train)\n",
    "y_predict_all = lr_all.predict_proba(x_test)[:, 1]\n",
    "auc_all = roc_auc_score(y_test, y_predict_all)\n",
    "\n",
    "# 不调整 penalty 参数\n",
    "lr_no_penalty = LogisticRegression(C=0.6, class_weight='balanced', solver='liblinear')\n",
    "lr_no_penalty.fit(x_train, y_train)\n",
    "y_predict_no_penalty = lr_no_penalty.predict_proba(x_test)[:, 1]\n",
    "auc_no_penalty = roc_auc_score(y_test, y_predict_no_penalty)\n",
    "\n",
    "# 不调整 C 参数\n",
    "lr_no_C = LogisticRegression(penalty='l1', class_weight='balanced', solver='liblinear')\n",
    "lr_no_C.fit(x_train, y_train)\n",
    "y_predict_no_C = lr_no_C.predict_proba(x_test)[:, 1]\n",
    "auc_no_C = roc_auc_score(y_test, y_predict_no_C)\n",
    "\n",
    "# 不调整 class_weight 参数\n",
    "lr_no_class_weight = LogisticRegression(penalty='l1', C=0.6, solver='liblinear')\n",
    "lr_no_class_weight.fit(x_train, y_train)\n",
    "y_predict_no_class_weight = lr_no_class_weight.predict_proba(x_test)[:, 1]\n",
    "auc_no_class_weight = roc_auc_score(y_test, y_predict_no_class_weight)\n",
    "\n",
    "# 计算提升百分比\n",
    "def calculate_improvement(default_auc, adjusted_auc):\n",
    "    return (adjusted_auc - default_auc) / default_auc * 100\n",
    "\n",
    "improvement_all = calculate_improvement(auc_default, auc_all)\n",
    "improvement_no_penalty = calculate_improvement(auc_default, auc_no_penalty)\n",
    "improvement_no_C = calculate_improvement(auc_default, auc_no_C)\n",
    "improvement_no_class_weight = calculate_improvement(auc_default, auc_no_class_weight)\n",
    "\n",
    "print('默认参数模型 AUC:', auc_default)\n",
    "print('所有参数调整后的模型 AUC:', auc_all, '提升百分比:', improvement_all)\n",
    "print('仅不调整 penalty 参数模型 AUC:', auc_no_penalty, '提升百分比:', improvement_no_penalty)\n",
    "print('仅不调整 C 参数模型 AUC:', auc_no_C, '提升百分比:', improvement_no_C)\n",
    "print('仅不调整 class_weight 参数模型 AUC:', auc_no_class_weight, '提升百分比:', improvement_no_class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然仅调整C参数模型之后AUC有些微下降，但是如果仅不调整C参数，模型AUC仍有所下降？没能搞明白？感觉和我理解的正则化有点冲突？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.7 使用标准化提升逻辑回归模型效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "部分数据，特别是与金额相关的数据，其取值跨度较大且值相对分散，以最近 12 个月取现金额均值 cashAmt_mean 为例，其统计信息为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/8cdadbd06a274dcab6fe57554fca7c2f667e830b0f7f455f9b8de5bdcc7bed02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "其平均值为 1471，最大值为 72633。标准差较大为 2892，表明数据较分散，这对于模型预测效果会有影响，为此我们需要对数据进行标准化处理。数据的标准化（normalization）是将全部数据平移到原点，实现数据的均值中心化，进一步按比例缩放，实现标准差单位化，使得数据不同变量维度的量纲一致，数据的分布更加规范化，为模型学习提供较规整的数据基础，提高优化过程的健壮性，比如，基于梯度下降的优化中能够快速收敛到最优值，降低不规范数据的干扰。\n",
    "\n",
    "\n",
    "在机器学习中对特征做归一化目的有:\n",
    "\n",
    "1,避免训练得到的模型权重过小,引起数值计算不稳定;\n",
    "\n",
    "2,使参数优化时能以较快的速度收敛.\n",
    "\n",
    "\n",
    "\n",
    "较常用的有 Z-score 标准化：\n",
    "\n",
    "Z-score 标准化，数据处理后符合标准正态分布，即均值为 0，标准差为 1 \n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/319787195e1542da8f35627f8bdd85ae3975dd6667ab40599114372f9c40a16c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 实训任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "对 data 中所有连续型的列 `continuous_columns` 进行 Z-score 标准化（直接在原始数据 data 上进行标准化）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，标准化后的数据，带入在之前已做过正则化的模型，相较于未标准化的情况，其 AUC 值有显著提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cashAmt_mean标准化后的均值： 8.406657906398548e-18\n",
      "cashAmt_mean标准化后的标准差： 1.00001056384524\n",
      "score: 0.8773915007413857\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.read_table('dataset11(不良记录).txt',sep='\\t')\n",
    "continuous_columns = ['age','cashTotalAmt','cashTotalCnt','monthCardLargeAmt','onlineTransAmt','onlineTransCnt','publicPayAmt','publicPayCnt','transTotalAmt','transTotalCnt','transCnt_non_null_months','transAmt_mean','transAmt_non_null_months','cashCnt_mean','cashCnt_non_null_months','cashAmt_mean','cashAmt_non_null_months','card_age', 'trans_total','total_withdraw', 'avg_per_withdraw','avg_per_online_spend', 'avg_per_public_spend', 'bad_record']\n",
    "\n",
    "# 对data中所有连续型的列进行Z-score标准化\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the continuous columns in the data\n",
    "data[continuous_columns] = scaler.fit_transform(data[continuous_columns])\n",
    "\n",
    "# 查看标准化后的数据的均值和标准差，以cashAmt_mean为例\n",
    "print('cashAmt_mean标准化后的均值：',data['cashAmt_mean'].mean())\n",
    "print('cashAmt_mean标准化后的标准差：',data['cashAmt_mean'].std())\n",
    "\n",
    "# 查看标准化后对模型的效果提升\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state = 33,stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "lr = LogisticRegression(penalty='l2',C=0.6,class_weight='balanced')\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# 查看模型预测结果\n",
    "y_predict = lr.predict_proba(x_test)[:,1]\n",
    "auc_score =roc_auc_score(y_test,y_predict)\n",
    "print('score:',auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.8 使用离散化提升逻辑回归模型效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在处理连续型数据时，为了便于分析需要将其离散化，即把数据放入一个个小区间中。若每个区间的间隔是相等的，这类离散化称作等距离散化。另外一种离散化叫做等频离散化，它是指每个区间的样本数相同。\n",
    "\n",
    "Pandas 提供了 `qcut()` 函数可实现等频离散化，自动分配每个面元的区间，其函数语法为：`qcut(x, q, duplicates='raise'...)`，其中：\n",
    "- `x` --  1 维 ndarray 或 Series，表示要划分的数据。\n",
    "- `q` -- 分位数，用来指定区间的数量，表示要划分为几组。\n",
    "- `duplicates` -- 如果分组的 bin 边缘不唯一，`duplicates` 为`‘raise’则`报告 ValueError，为`‘drop’`则直接删除非唯一值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 实训任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "对data中所有数值连续型的列 `continuous_columns` 利用 `qcut` 进行等频离散化，将每一列都离散为5组，设置 `duplicates` 为 `drop` 以删除非唯一值（直接在原始数据 data 中进行离散化）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，离散化后的数据，带入原始模型中，相较于未离散化的情况，其AUC值也有了明显提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age                       card_age     cashAmt_mean  \\\n",
      "0    (0.176, 0.909]  (-1.4649999999999999, -0.876]  (-0.51, -0.457]   \n",
      "1    (0.176, 0.909]              (-0.406, 0.00474]  (-0.51, -0.457]   \n",
      "2    (0.176, 0.909]               (-0.876, -0.406]  (-0.51, -0.457]   \n",
      "3    (0.176, 0.909]               (-0.876, -0.406]  (0.286, 24.604]   \n",
      "4  (-2.021, -0.922]  (-1.4649999999999999, -0.876]  (-0.51, -0.457]   \n",
      "\n",
      "  cashAmt_non_null_months      cashCnt_mean cashCnt_non_null_months  \\\n",
      "0        (-0.815, -0.429]  (-0.872, -0.459]        (-0.817, -0.432]   \n",
      "1        (-0.815, -0.429]  (-0.872, -0.459]        (-0.817, -0.432]   \n",
      "2        (-0.815, -0.429]  (-0.872, -0.459]        (-0.817, -0.432]   \n",
      "3        (-0.815, -0.429]   (0.594, 20.627]        (-0.817, -0.432]   \n",
      "4        (-0.815, -0.429]  (-0.872, -0.459]        (-0.817, -0.432]   \n",
      "\n",
      "       cashTotalAmt      cashTotalCnt  inCourt  isBlackList  ...  sex_2  \\\n",
      "0   (-0.34, -0.259]  (-0.535, -0.431]        0            0  ...      1   \n",
      "1   (-0.34, -0.259]  (-0.535, -0.431]        0            0  ...      0   \n",
      "2   (-0.34, -0.259]  (-0.535, -0.431]        0            0  ...      1   \n",
      "3  (0.0764, 33.597]   (-0.227, 0.286]        0            0  ...      0   \n",
      "4   (-0.34, -0.259]  (-0.535, -0.431]        0            0  ...      0   \n",
      "\n",
      "   CityId_1 CityId_2  CityId_3        trans_total     total_withdraw  \\\n",
      "0         1        0         0   (-0.129, -0.127]   (-0.183, -0.181]   \n",
      "1         1        0         0   (-0.129, -0.127]   (-0.183, -0.181]   \n",
      "2         0        1         0   (-0.127, -0.121]   (-0.183, -0.181]   \n",
      "3         0        0         1  (-0.0778, 89.189]  (-0.0278, 64.504]   \n",
      "4         0        1         0   (-0.127, -0.121]   (-0.183, -0.181]   \n",
      "\n",
      "   avg_per_withdraw avg_per_online_spend avg_per_public_spend       bad_record  \n",
      "0  (-0.444, -0.155]   (-38.525, -0.0376]    (-36.07, -0.0568]  (-0.323, 7.482]  \n",
      "1  (-0.444, -0.155]      (0.0817, 0.117]    (-36.07, -0.0568]  (-0.323, 7.482]  \n",
      "2  (-0.444, -0.155]   (-38.525, -0.0376]     (-0.0349, 0.176]  (-0.323, 7.482]  \n",
      "3   (0.273, 30.811]      (0.117, 45.123]    (-36.07, -0.0568]  (-0.323, 7.482]  \n",
      "4  (-0.444, -0.155]    (-0.0376, 0.0817]     (-0.0349, 0.176]  (-0.323, 7.482]  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "score: 0.910711509317238\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_table('dataset12.txt',sep='\\t')\n",
    "continuous_columns = ['age','cashTotalAmt','cashTotalCnt','monthCardLargeAmt','onlineTransAmt','onlineTransCnt','publicPayAmt','publicPayCnt','transTotalAmt','transTotalCnt','transCnt_non_null_months','transAmt_mean','transAmt_non_null_months','cashCnt_mean','cashCnt_non_null_months','cashAmt_mean','cashAmt_non_null_months','card_age', 'trans_total', 'total_withdraw', 'avg_per_withdraw','avg_per_online_spend', 'avg_per_public_spend', 'bad_record']\n",
    "\n",
    "# 对data中数值连续型的列进行等频离散化，将每一列都离散为5个组。\n",
    "for col in continuous_columns:\n",
    "    data[col] = pd.qcut(data[col], q=5, duplicates='drop')\n",
    "\n",
    "\n",
    "# 查看离散化后的数据\n",
    "print(data.head())\n",
    "\n",
    "# 查看离散化后对模型的效果提升\n",
    "# 先对各离散组进行One-Hot处理\n",
    "data=pd.get_dummies(data)\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state = 33,stratify=y)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lr = LogisticRegression(penalty='l2',C=0.6,class_weight='balanced')\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# 查看模型预测结果\n",
    "y_predict = lr.predict_proba(x_test)[:,1]\n",
    "score_auc = roc_auc_score(y_test,y_predict)\n",
    "print('score:',score_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 总结：各种方案的性能比较 （注意比较的公平性）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 请在此处作答：1.模型参数调优后score：0.8776952288366846；2.使用标准化提升逻辑回归模型效果score: 0.8773915007413857;3.使用离散化提升逻辑回归模型效果score: 0.910711509317238\n",
    "三种方案提升逻辑回归模型效果对比可得，单独采用离散化提升效果最好。\n",
    "\n",
    "\n",
    "但是， 模型参数调优是指选择最佳的超参数组合，以获得最优的模型性能；特征标准化是将连续特征进行缩放，使其具有零均值和单位方差；特征离散化是将连续特征划分为不同的区间或类别。\n",
    "\n",
    "\n",
    "显然，标准化和离散化其实处理的是不同方面的特征，因此我尝试了同时采用以上三种方式来提升模型效果。\n",
    "\n",
    "\n",
    "另外，性能比较不仅在于采用何种方式提升最高，例如采用标准化提升逻辑回归模型效果，和模型参数调优提升效果相近，但是模型参数调优仅需要调整超参数，占用运算和存储资源可能更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳模型 AUC: 0.8876435173213665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_table('dataset11(不良记录).txt', sep='\\t')\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "\n",
    "# 划分训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=33, stratify=y)\n",
    "\n",
    "# 离散化\n",
    "discretizer = KBinsDiscretizer(n_bins=10, encode='onehot', strategy='uniform')\n",
    "x_train_binned = discretizer.fit_transform(x_train)\n",
    "x_test_binned = discretizer.transform(x_test)\n",
    "\n",
    "# 模型参数调优\n",
    "param_grid = {'C': [0.1, 0.6, 1, 10], 'penalty': ['l1', 'l2'], 'class_weight': ['balanced', None]}\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(x_train_binned, y_train)\n",
    "\n",
    "# 最佳模型\n",
    "best_lr = grid_search.best_estimator_\n",
    "y_predict = best_lr.predict_proba(x_test_binned)[:, 1]\n",
    "\n",
    "# 计算AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "test_auc = roc_auc_score(y_test, y_predict)\n",
    "print('最佳模型 AUC:', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "然而，这个模型的AUC值竟然比之前的模型还要低。标准化将数据缩放到相同的范围，但离散化会将连续特征转换为离散特征，这可能会导致信息丢失。特别是当离散化的分箱数较少时，可能会丢失特征的细节信息。此外，同时进行标准化和离散化可能会导致特征冗余。例如，标准化后的特征和离散化后的特征可能包含重复的信息，这会增加模型的复杂性，导致过拟合或欠拟合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳模型 AUC: 0.8776330375428112\n"
     ]
    }
   ],
   "source": [
    "#选择了另外的方法，仅进行标准化和参数调优。\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_table('dataset11(不良记录).txt', sep='\\t')\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "\n",
    "# 划分训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=33, stratify=y)\n",
    "\n",
    "# 标准化\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# 模型参数调优\n",
    "param_grid = {'C': [0.1, 0.6, 1, 10], 'penalty': ['l1', 'l2'], 'class_weight': ['balanced', None]}\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(x_train_scaled, y_train)\n",
    "\n",
    "# 最佳模型\n",
    "best_lr = grid_search.best_estimator_\n",
    "y_predict = best_lr.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# 计算AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "test_auc = roc_auc_score(y_test, y_predict)\n",
    "print('最佳模型 AUC:', test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳模型 AUC: 0.8876406532486224\n"
     ]
    }
   ],
   "source": [
    "#仅进行离散化和超参数调优\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_table('dataset11(不良记录).txt', sep='\\t')\n",
    "y = data['Default'].values\n",
    "x = data.drop(['Default'], axis=1).values\n",
    "\n",
    "# 划分训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=33, stratify=y)\n",
    "\n",
    "# 离散化\n",
    "discretizer = KBinsDiscretizer(n_bins=10, encode='onehot', strategy='uniform')\n",
    "x_train_binned = discretizer.fit_transform(x_train)\n",
    "x_test_binned = discretizer.transform(x_test)\n",
    "\n",
    "# 模型参数调优\n",
    "param_grid = {'C': [0.1, 0.6, 1, 10], 'penalty': ['l1', 'l2'], 'class_weight': ['balanced', None]}\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "grid_search.fit(x_train_binned, y_train)\n",
    "\n",
    "# 最佳模型\n",
    "best_lr = grid_search.best_estimator_\n",
    "y_predict = best_lr.predict_proba(x_test_binned)[:, 1]\n",
    "\n",
    "# 计算AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "test_auc = roc_auc_score(y_test, y_predict)\n",
    "print('最佳模型 AUC:', test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
